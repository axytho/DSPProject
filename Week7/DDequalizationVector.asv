M = 4;
channelChangeRate = 0.0001;
Hk = [ones(77, 1) * (0.5+0.6j)];%; ones(300, 1) * (0.52+0.59j); ones(400, 1) * (0.51+0.55j)];
bits = randi([0, 1], 77*log2(M), 1);
Xk = qam_mod(bits, M);
Nk = 0.3;
Yknoiseless = Hk .* Xk;
SNR = 30050;
Yk = awgn(Yknoiseless, SNR);
% 7.1.2
delta = 0.01;
Wk = 1/conj(Hk(1)) + delta; %initial value
iterations = 76;
% Implementing normalized LMS
WkMatrix = [Wk; zeros(iterations,1)];
%mu = 0.9;
mu = linspace(0.5,1,6);
for i = 1:length(mu)
    for j = 1:iterations
        XkEstimated = qam_mod(qam_demod(conj(WkMatrix(j)) * Yk(j+1), M), M);
        WkMatrix(j+1) = WkMatrix(j) + mu(i) * Yk(j+1) / abs(Yk(j+1))^2 *(XkEstimated - conj(WkMatrix(j)) * Yk(j+1));
    end
    error = abs(conj(WkMatrix) - 1./( Hk(1:(iterations+1))));
    plot(1:length(WkMatrix), error,'displayname',num2mu');
    title("Error signals over the number of iterations");
    hold on;
end
legend show;

    
%for i=1:len(mu)
%    plot(1:length(WkMatrix(i)), error);
%    hold on
%end

% Implementing normalized LMS
%{
WkMatrix = [Wk; zeros(iterations, 1)];
mu = 0.9;
for j =1:iterations
    XkEstimated = qam_mod(qam_demod(conj(WkMatrix(j)) * Yk(j+1), M), M);
    WkMatrix(j+1) = WkMatrix(j) + mu * Yk(j+1) / abs(Yk(j+1))^2 *(XkEstimated - conj(WkMatrix(j)) * Yk(j+1));
end
error = abs(conj(WkMatrix) - 1./( Hk(1:(iterations+1))));
Hk
plot(1:length(WkMatrix), error);
%}